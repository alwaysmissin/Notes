# 类神经网络训练
## 局部最小与马鞍点(Local Minimum and Saddle Point)
- 当梯度小时，可能被困在的点(critical point)是
	- 局部最小
	- 马鞍点
![](https://s2.loli.net/2022/07/21/ZUOzloM9mGaeRvk.png)

## 批量与动量(Batch and Momentum)
### Batch
- 在更新参数时，并不是一次对所有的数据算出后，再更新参数，而是将所有的数据分成多份，对参数进行多次更新
![image.png](https://s2.loli.net/2022/07/21/eDngAEBLUlXbHZ3.png)
- 对于处理的时间：
	- 由于使用GPU的**平行运算**进行机器学习，在时间上，一次处理多个数据更加高效
![image.png](https://s2.loli.net/2022/07/21/hHrPtcnpFRWxbsO.png)
- Noisy的Gradient可以帮助训练
![image.png](https://s2.loli.net/2022/07/21/OiDWpMnwKx3SguQ.png)
- 纵轴代表精确度
- 横轴代表Batch Size

- 一种可能的解释：
![image.png](https://s2.loli.net/2022/07/21/XMOJ6aKfpITbdWv.png)
1. Full Batch：在梯度为0的点（local minimum/saddle point）停下后，无法继续更新
2. Small Batch：不同的Batch算出来的Loss Function不同，可能在前一个Batch卡住，后一个Batch可以继续计算
![](https://s2.loli.net/2022/07/21/dYIpSfRzXWC6OMB.png)
### Momentum(动量)
- 利用前一个计算出的梯度，提供一定的惯性（在下一次计算梯度时，与其进行合成，才成为一个新的梯度），在一定程度上可以冲出局部最小(local minimum)
![](https://s2.loli.net/2022/07/21/9uioAErZG6NFJaf.png)
![](https://s2.loli.net/2022/07/21/kNbQSLCe4Jntu8Y.png)
## 自适应学习率(Adaptive Learning Rate)
- 出现问题的两种情况：
	- 在“峡谷”两边来回
	- “平原”处更新缓慢
![image.png](https://s2.loli.net/2022/07/21/BkLxmNMc2Sz8r4G.png)
![image.png](https://s2.loli.net/2022/07/21/4g3MquvEBS5fzDb.png)
- 自适应的学习率：在平坦的地方，学习率大；在陡峭的地方，学习率小

## 对于Loss Function的选择