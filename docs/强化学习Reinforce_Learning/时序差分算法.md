# 时序差分算法
[时序差分算法代码实现](https://github.com/alwaysmissin/ReinforcementLearning/blob/master/4TD.ipynb)
> [!概念：在线策略学习与离线策略学习]
> - 在线策略学习：
> 	- 要求**使用在当前策略下采样的到的样本进行学习**
> 	- 一旦策略被更新，当前的样本就被放弃了
> 	- 类比在水龙头下直接使用自来水洗手
> - 离线策略学习：
> 	- 使用经验回放池将之前采样得到的样本**收集起来再次利用**
> 	- 类比使用脸盆接水后洗手
> 因此，离线学习能够更好的利用历史数据，并且具有更小的样本复杂度
## 时序差分
- 结合了蒙特卡洛和动态规划的思想
	- 蒙特卡洛方法：可以从样本数据中学习，**无需事先知道环境**
	- 动态规划：根据贝尔曼方程的思想，利用后续状态的价值估计来更新当前状态的价值
- 对于蒙特卡洛方法的优化：
	- 蒙特卡洛方法：$V(s_t)\leftarrow V(s_t)+\alpha [G_t-V(s_t)]$
	
		- 我们可以将$\alpha$取值为一个常数，使更新方式不再像蒙特卡洛方法严格的取期望，称其为**学习率**
		- 在蒙特卡洛方法中，需要采集到整个序列才能进行更新
			- 而**时序差分算法直接使用当前获得的奖励加上下一个状态的价值估计来作为在当前状态可以获得的回报**，得到公式：$$V\left(s_{t}\right) \leftarrow V\left(s_{t}\right)+\alpha\left[r_{t}+\gamma V\left(s_{t+1}\right)-V\left(s_{t}\right)\right]$$
> 可以使用$r_{t}+\gamma V\left(s_{t+1}\right)$替代$Q_t$的原因：
> ![](https://raw.githubusercontent.com/alwaysmissin/picgo/main/20221130215108.png)
## Sarsa算法
>[!提出]
>- 在不知道奖励函数和状态转移函数的情况下如何进行策略提升？？？
>	- 答案：**直接使用时序差分算法**来进行估计状态价值函数$Q(s,a)$$$Q\left(s_{t}, a_{t}\right) \leftarrow Q\left(s_{t}, a_{t}\right)+\alpha\left[r_{t}+\gamma Q\left(s_{t+1}, a_{t+1}\right)-Q\left(s_{t}, a_{t}\right)\right]$$
>	- 接下来使用**贪婪算法**来选取在某个状态下动作价值最大的动作：$arg\max _{a} Q(s, a)$
- 完整的流程：
	- 使用**贪婪算法**根据动作价值选取动作来与环境进行交互
	- 根据得到的数据用[[时序差分算法#时序差分|时序差分算法]]更新动作价值估计
- 两个**问题**与对应的**解决**：
	- 如果要使用时序差分算法来准确估计策略的状态价值函数，我们需要用**极大量的样本**来进行更新
		- 直接使用一些样本来评估策略
			- 策略提升可以在策略评估未完全完成时候进行
	- 如果一直使用贪婪策略来选取动作，可能导致有些动作一直未被选取
		- 使用$\epsilon$贪婪算法
	 由此得到Sarsa算法
- Sarsa算法的具体流程：
	- 流程图：![](https://raw.githubusercontent.com/alwaysmissin/picgo/main/20221130221443.png)
	- 代码：
	```python
	state = env.reset()
	action = agent.take_action(state)
	done = False
	while not done:
		next_state, reward, done = env.step(action)
		next_action = agent.take_action(next_state)
		episode_return += reward
		agent.update(state, action, reward, next_state, next_action)
		state = next_state
		action = next_action
	```
