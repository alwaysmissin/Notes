# 基础操作
- gym库中自带大量的环境
- 简单的强化学习框架：
```python
import gym
env = gym.make("Taxi-v3")
observation = env.reset()
# load_agent()为自行定义的函数
agent = load_agent()
for step in range(100):
	action = agent(observation)
	observation, reward, done, info = ent.step(action)
```
- 使用这些简单的环境，可以在短时间内看到效果
> 注意：如果绘制了实验的图形界面窗口，那么关闭该窗口的最佳方式是调用`env.close()`
> 试图直接关闭图形界面窗口可能会导致内存不能释放，甚至会导致死机
- gym库中大部分小游戏都可以用一个普通的实数或向量来表示动作
	- `env.action_space.sample()`可以随机从动作空间中返回一个动作
- `env.step(action)`
	- 四个返回值
		- `observation`
			- 状态信息
			- 是在游戏中观测到的屏幕像素值或盘面状态描述信息
		- `reward`
			- 奖励值
			- 动作提交后能够获得的奖励值
			- 奖励值因游戏的不同而不同
			- 原则：
				- 对完成游戏有帮助的动作会获得比较高的奖励值
		- `done`
			- 表示游戏是否已经完成
			- 如果完成了，就需要重置游戏，并且开始一个新的回合
		- `info`
			- 一些比较原始的用于诊断和调试的信息
			- 或许对训练有帮助
	- 一次完成一个完整的$S \rightarrow A \rightarrow R \rightarrow S^\prime$
		- 我们只需要不断观测这样的过程，并让智能体在其中用相应的算法完成训练，就可以的得到一个高质量的强化学习模型
- 观测空间与动作空间
	- 观测空间：`env.observation_space`
	- 动作空间：`env.action_space`
- 空间分类：
	- 离散空间：`gym.spaces.Discrete`
	- 连续空间：`gym.spaces.Box`
- [检测环境是否可用](https://www.gymlibrary.dev/content/api/#checking-api-conformity)：
```python
from gym.utils.env_checker import check_env

chevk_env(env[, warn = True, skip_render = False])
```
- [空间](https://www.gymlibrary.dev/content/api/#spaces)：
	- Box: n维的连续空间
		- 可以设置上下边界
	- Discrete: 离散的状态
	- Dict
	- Tuple
	- MultiBinary
	- MultiDiscrete
- [包装器](https://www.gymlibrary.dev/content/api/#checking-api-conformity)
	- 便于修改现有的环境
	- 功能：
		- 改变行为->`ActionWrapper`
		- 改变观察->`ObservationWrapper`
		- 改变奖励->`RewardWrapper`
	- 使用`.unwrapped`获得未包装的环境
	- gym提供的常用包装器：
		- `TimeLimit`
		- `ClipAction`
		- `RescaleAction`
		- `TimeAwareObservation`
- [Play within an environment](https://www.gymlibrary.dev/content/api/#playing-within-an-environment)
## MounatinCar-v0 例子
- 动作空间与观测空间
```python
import gym  
env = gym.make('MountainCar-v0')  
print('观测空间 = {}'.format(env.observation_space))  
print('动作空间 = {}'.format(env.action_space))  
print('观测范围 = {} ~ {}'.format(env.observation_space.low, env.observation_space.high))  
print('动作数 = {}'.format(env.action_space.n))  
  
# 观测空间 = Box([-1.2  -0.07], [0.6  0.07], (2,), float32)
# 动作空间 = Discrete(3)
# 观测范围 = [-1.2  -0.07] ~ [0.6  0.07]
# 动作数 = 3
```

- 智能体


## 总结
- 取出环境：`env = gym.make(环境名)`
- 初始化环境：`env.reset()`
- 执行一步环境：`env.step()`
- 显示环境：`env.render()`
- 关闭环境：`env.close()`
官方文档库：[Gym Documentation](https://www.gymlibrary.dev)