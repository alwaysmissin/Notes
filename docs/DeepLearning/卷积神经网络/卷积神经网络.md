---
tags:
  - 动手学深度学习
Reference: 动手学深度学习
---
# 卷积神经网络
## 从全连接到卷积
### 不变性
- 合理的假设: 无论那种方法找到这个物体, 都与物体的位置无关
- 一个是个与计算机视觉的神经网络架构:
	1. **平移不变性(translation invariance)**: 无论检测对象出现在图像中的哪一个位置, 神经网络的前面几层应该对相同的图像区域具有相似的反应
	2. **局部性(locality)**: 神经网络的前面几层应该只探索输入图像中的局部区域, 而不过度在意图像中相关较远区域的关系

### 多层感知机的限制
- 全连接层的形式化表示: $$\begin{aligned}
{[\mathbf{H}]_{i, j} } & =[\mathbf{U}]_{i, j}+\sum_k \sum_l[\mathbf{W}]_{i, j, k, l}[\mathbf{X}]_{k, l} \\
& =[\mathbf{U}]_{i, j}+\sum_a \sum_b[\mathbf{V}]_{i, j, a, b}[\mathbf{X}]_{i+a, j+b}
\end{aligned}$$
	- $[\mathbf{X}]_{i, j}$和$[\mathbf{H}]_{i, j}$: 分别表示输入图像和隐藏表示中位置$(i, j)$处的像素
	- $\mathbf{W}$和$\mathbf{V}$: 四阶权重张量
	- $\mathbf{U}$: 包含偏置参数

#### 平移不变性
- 这意味着检测对象在输入$\mathbf{X}$中的平移, 应该仅导致隐藏表示$\mathbf{H}$中的平移
	- **即**: $\mathbf{V}$和$\mathbf{U}$实际上不依赖于$(i,j)$的值, 即$[\mathbf{V}]_{i, j, a, b} =  [\mathbf{V}]_{a, b}$,
	- $\mathbf{H}$可简化为:$$[\mathbf{H}]_{i, j}=[\mathbf{U}]_{i, j}+\sum_a \sum_b[\mathbf{V}]_{i, j, a, b}[\mathbf{X}]_{i+a, j+b}$$
**不再依赖于图像中的位置**

#### 局部性
- 为了收集用来训练参数$[\mathbf{H}]_{i, j}$的相关信息, 我们**不应该偏离距离$(i,j)$很远的地方**
	- 即: 在$|a|>\Delta$或$|b|>\Delta$的范围之外, 可以设置$[\mathbf{V}]_{a, b}=0$
	- $\mathbf{H}$可重写为:$$[\mathbf{H}]_{i, j}=u+\sum_{a=-\Delta}^{\Delta} \sum_{b=-\Delta}^{\Delta}[\mathbf{V}]_{a, b}[\mathbf{X}]_{i+a, j+b}$$

## 图像卷积
### 互相关运算
- 卷积层 -> 互相关运算: 输入张量和核张量通过互相关运算产生输出张量![image.png](https://raw.githubusercontent.com/alwaysmissin/picgo/main/20231025124615.png)

### 特征映射和感受野
- 输出的卷积层有时候被称为特征映射, 因此, 他可以被视为一个输入映射到下一层的空间维度的转换器
- 感受野: 值在前向传播期间可能影响x计算的所有元素
	- 感受野可能大于输入的实际大小

## 填充和步幅

## 多输入多输出通道
### 多输入通道
- 需要构造一个与输入数据具有相同输入通道数的卷积核
- 

## 汇聚层
- 目的:
	- 降低卷积层对位置的敏感性
	- 降低对空间降采样表示的敏感性

### 最大汇聚层与平均汇聚层
我们通常计算汇聚窗口中所有元素的最大值或平均值, 称为最大汇聚层或平均汇聚层

## 参考书目
- 动手学深度学习-pytorch